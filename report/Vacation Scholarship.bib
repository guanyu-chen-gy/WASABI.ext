@article{balocchiCrimePhiladelphiaBayesian2023,
  title = {Crime in {{Philadelphia}}: {{Bayesian Clustering}} with {{Particle Optimization}}},
  shorttitle = {Crime in {{Philadelphia}}},
  author = {Balocchi, Cecilia and Deshpande, Sameer K. and George, Edward I. and Jensen, Shane T.},
  year = {2023},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {118},
  number = {542},
  pages = {818--829},
  publisher = {ASA Website},
  issn = {0162-1459},
  doi = {10.1080/01621459.2022.2156348},
  urldate = {2025-08-27},
  abstract = {Accurate estimation of the change in crime over time is a critical first step toward better understanding of public safety in large urban environments. Bayesian hierarchical modeling is a natural way to study spatial variation in urban crime dynamics at the neighborhood level, since it facilitates principled ``sharing of information'' between spatially adjacent neighborhoods. Typically, however, cities contain many physical and social boundaries that may manifest as spatial discontinuities in crime patterns. In this situation, standard prior choices often yield overly smooth parameter estimates, which can ultimately produce mis-calibrated forecasts. To prevent potential over-smoothing, we introduce a prior that partitions the set of neighborhoods into several clusters and encourages spatial smoothness within each cluster. In terms of model implementation, conventional stochastic search techniques are computationally prohibitive, as they must traverse a combinatorially vast space of partitions. We introduce an ensemble optimization procedure that simultaneously identifies several high probability partitions by solving one optimization problem using a new local search strategy. We then use the identified partitions to estimate crime trends in Philadelphia between 2006 and 2017. On simulated and real data, our proposed method demonstrates good estimation and partition selection performance. Supplementary materials for this article are available online.},
  keywords = {Bayesian model averaging,Boundary detection,Spatial clustering,Spatial smoothness,Urban analytics,Variational inference},
  file = {/Users/chenguanyu/Zotero/storage/S6WE5NF3/Balocchi et al. - 2023 - Crime in Philadelphia Bayesian Clustering with Particle Optimization.pdf}
}

@misc{balocchiUnderstandingUncertaintyBayesian2025,
  title = {Understanding Uncertainty in {{Bayesian}} Cluster Analysis},
  author = {Balocchi, Cecilia and Wade, Sara},
  year = {2025},
  month = jun,
  number = {arXiv:2506.16295},
  eprint = {2506.16295},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.16295},
  urldate = {2025-08-26},
  abstract = {The Bayesian approach to clustering is often appreciated for its ability to provide uncertainty in the partition structure. However, summarizing the posterior distribution over the clustering structure can be challenging, due the discrete, unordered nature and massive dimension of the space. While recent advancements provide a single clustering estimate to represent the posterior, this ignores uncertainty and may even be unrepresentative in instances where the posterior is multimodal. To enhance our understanding of uncertainty, we propose a WASserstein Approximation for Bayesian clusterIng (WASABI), which summarizes the posterior samples with not one, but multiple clustering estimates, each corresponding to a different part of the space of partitions that receives substantial posterior mass. Specifically, we find such clustering estimates by approximating the posterior distribution in a Wasserstein distance sense, equipped with a suitable metric on the partition space. An interesting byproduct is that a locally optimal solution to this problem can be found using a k-medoids-like algorithm on the partition space to divide the posterior samples into different groups, each represented by one of the clustering estimates. Using both synthetic and real datasets, we show that our proposal helps to improve the understanding of uncertainty, particularly when the data clusters are not well separated or when the employed model is misspecified.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {/Users/chenguanyu/Zotero/storage/WXCJJRQ9/Balocchi and Wade - 2025 - Understanding uncertainty in Bayesian cluster analysis.pdf;/Users/chenguanyu/Zotero/storage/RALNGRB4/2506.html}
}

@article{binderBayesianClusterAnalysis1978,
  title = {Bayesian {{Cluster Analysis}}},
  author = {Binder, D. A.},
  year = {1978},
  journal = {Biometrika},
  volume = {65},
  number = {1},
  eprint = {2335273},
  eprinttype = {jstor},
  pages = {31--38},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2335273},
  urldate = {2025-08-27},
  abstract = {A parametric model for partitioning individuals into mutually exclusive groups is given. A Bayesian analysis is applied and a loss structure imposed. A model-dependent definition of a similarity matrix is proposed and estimates based on this matrix are justified in a decision-theoretic framework. Some existing cluster analysis techniques are derived as special limiting cases. The results of the procedure applied to two data sets are compared with other analyses.},
  file = {/Users/chenguanyu/Zotero/storage/NE4RRHM4/Binder - 1978 - Bayesian Cluster Analysis.pdf}
}

@inproceedings{canasLearningProbabilityMeasures2012,
  title = {Learning {{Probability Measures}} with Respect to {{Optimal Transport Metrics}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Canas, Guillermo and Rosasco, Lorenzo},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-08-28},
  abstract = {We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.},
  file = {/Users/chenguanyu/Zotero/storage/GB8PQMMI/Canas and Rosasco - 2012 - Learning Probability Measures with respect to Optimal Transport Metrics.pdf}
}

@misc{ClassBayesianNonparametric,
  title = {On a {{Class}} of {{Bayesian Nonparametric Estimates}}: {{I}}. {{Density Estimates}} on {{JSTOR}}},
  eprint = {2241054},
  eprinttype = {jstor},
  urldate = {2025-08-29},
  howpublished = {https://www.jstor.org/stable/2241054?seq=1},
  file = {/Users/chenguanyu/Zotero/storage/XW8UM8KV/2241054.html}
}

@article{corradinBNPmixPackageBayesian2021,
  title = {{{BNPmix}}: {{An R Package}} for {{Bayesian Nonparametric Modeling}} via {{Pitman-Yor Mixtures}}},
  shorttitle = {{{BNPmix}}},
  author = {Corradin, Riccardo and Canale, Antonio and Nipoti, Bernardo},
  year = {2021},
  month = nov,
  journal = {Journal of Statistical Software},
  volume = {100},
  pages = {1--33},
  issn = {1548-7660},
  doi = {10.18637/jss.v100.i15},
  urldate = {2025-08-29},
  abstract = {BNPmix is an R package for Bayesian nonparametric multivariate density estimation, clustering, and regression, using Pitman-Yor mixture models, a flexible and robust generalization of the popular class of Dirichlet process mixture models. A variety of model specifications and state-of-the-art posterior samplers are implemented. In order to achieve computational efficiency, all sampling methods are written in C++ and seamless integrated into R by means of the Rcpp and RcppArmadillo packages. BNPmix exploits the ggplot2 capabilities and implements a series of generic functions to plot and print summaries of posterior densities and induced clustering of the data.},
  copyright = {Copyright (c) 2021 Riccardo Corradin, Antonio Canale, Bernardo Nipoti},
  langid = {english},
  keywords = {Bayesian nonparametric mixture,C,clustering,density estimation,importance conditional sampler,marginal sampler,slice sampler},
  file = {/Users/chenguanyu/Zotero/storage/Z6WSLDZ2/Corradin et al. - 2021 - BNPmix An R Package for Bayesian Nonparametric Modeling via Pitman-Yor Mixtures.pdf}
}

@inproceedings{dahl10ModelBasedClustering2006,
  title = {10 {{Model-Based Clustering}} for {{Expression Data}} via a {{Dirichlet Process Mixture Model}}},
  author = {Dahl, David B.},
  year = {2006},
  urldate = {2025-08-27},
  abstract = {This chapter describes a clustering procedure for microarray expression data based on a well-defined statistical model, specifically, a conjugate Dirichlet process mixture model. The clustering algorithm groups genes whose latent variables governing expression are equal, that is, genes belonging to the same mixture component. The model is fit with Markov chain Monte Carlo and the computational burden is eased by exploiting conjugacy. This chapter introduces a method to get a point estimate of the true clustering based on least-squares distances from the posterior probability that two genes are clustered. Unlike ad hoc clustering methods, the model provides measures of uncertainty about the clustering. Further, the model automatically estimates the number of clusters and quantifies uncertainty about this important parameter. The method is compared to other clustering methods in a simulation study. Finally, the method is demonstrated with actual microarray data.},
  note = {[TLDR] This chapter introduces a method to get a point estimate of the true clustering based on least-squares distances from the posterior probability that two genes are clustered, and is compared to other clustering methods in a simulation study.},
  file = {/Users/chenguanyu/Zotero/storage/348ZT8TB/Dahl - 2006 - 10 Model-Based Clustering for Expression Data via a Dirichlet Process Mixture Model.pdf}
}

@article{dahlSearchAlgorithmsLoss2022,
  title = {Search {{Algorithms}} and {{Loss Functions}} for {{Bayesian Clustering}}},
  author = {Dahl, David B. and Johnson, Devin J. and M{\"u}ller, Peter},
  year = {2022},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {31},
  number = {4},
  pages = {1189--1201},
  publisher = {ASA Website},
  issn = {1061-8600},
  doi = {10.1080/10618600.2022.2069779},
  urldate = {2025-08-26},
  abstract = {We propose a randomized greedy search algorithm to find a point estimate for a random partition based on a loss function and posterior Monte Carlo samples. Given the large size and awkward discrete nature of the search space, the minimization of the posterior expected loss is challenging. Our approach is a stochastic search based on a series of greedy optimizations performed in a random order and is embarrassingly parallel. We consider several loss functions, including Binder loss and variation of information. We note that criticisms of Binder loss are the result of using equal penalties of misclassification and we show an efficient means to compute Binder loss with potentially unequal penalties. Furthermore, we extend the original variation of information to allow for unequal penalties and show no increased computational costs. We provide a reference implementation of our algorithm. Using a variety of examples, we show that our method produces clustering estimates that better minimize the expected loss and are obtained faster than existing methods. Supplementary materials for this article are available online.},
  keywords = {Bayesian nonparametrics,Binder loss,Cluster estimation,Random partition models,Stochastic optimization,Variation of information},
  file = {/Users/chenguanyu/Zotero/storage/PJ3C63ET/Dahl et al. - 2022 - Search Algorithms and Loss Functions for Bayesian Clustering.pdf}
}

@article{hartiganAlgorithm136KMeans1979,
  title = {Algorithm {{AS}} 136: {{A K-Means Clustering Algorithm}}},
  shorttitle = {Algorithm {{AS}} 136},
  author = {Hartigan, J. A. and Wong, M. A.},
  year = {1979},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {28},
  number = {1},
  eprint = {2346830},
  eprinttype = {jstor},
  pages = {100--108},
  publisher = {[Royal Statistical Society, Oxford University Press]},
  issn = {0035-9254},
  doi = {10.2307/2346830},
  urldate = {2025-08-26},
  file = {/Users/chenguanyu/Zotero/storage/E8GKHVFR/Hartigan and Wong - 1979 - Algorithm AS 136 A K-Means Clustering Algorithm.pdf}
}

@article{hennigWhatAreTrue2015,
  title = {What Are the True Clusters?},
  author = {Hennig, Christian},
  year = {2015},
  month = oct,
  journal = {Pattern Recognition Letters},
  series = {Philosophical {{Aspects}} of {{Pattern Recognition}}},
  volume = {64},
  pages = {53--62},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2015.04.009},
  urldate = {2025-08-27},
  abstract = {Constructivist philosophy and Hasok Chang's active scientific realism are used to argue that the idea of ``truth'' in cluster analysis depends on the context and the clustering aims. Different characteristics of clusterings are required in different situations. Researchers should be explicit about on what requirements and what idea of ``true clusters'' their research is based, because clustering becomes scientific not through uniqueness but through transparent and open communication. The idea of ``natural kinds'' is a human construct, but it highlights the human experience that the reality outside the observer's control seems to make certain distinctions between categories inevitable. Various desirable characteristics of clusterings and various approaches to define a context-dependent truth are listed, and I discuss what impact these ideas can have on the comparison of clustering methods, and the choice of a clustering methods and related decisions in practice.},
  keywords = {Active scientific realism,Categorization,Constructivism,Mixture models,Natural kinds,Variable selection},
  file = {/Users/chenguanyu/Zotero/storage/L5J2KUND/Hennig - 2015 - What are the true clusters.pdf;/Users/chenguanyu/Zotero/storage/37FHTQ3Q/S0167865515001269.html}
}

@article{hubertComparingPartitions1985,
  title = {Comparing Partitions},
  author = {Hubert, Lawrence and Arabie, Phipps},
  year = {1985},
  month = dec,
  journal = {Journal of Classification},
  volume = {2},
  number = {1},
  pages = {193--218},
  issn = {1432-1343},
  doi = {10.1007/BF01908075},
  urldate = {2025-08-27},
  abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between {\textpm}1.},
  langid = {english},
  keywords = {Consensus indices,Measures of agreement,Measures of association},
  file = {/Users/chenguanyu/Zotero/storage/VHXU4BJ5/Hubert and Arabie - 1985 - Comparing partitions.pdf}
}

@article{jainDataClustering502010,
  title = {Data Clustering: 50 Years beyond {{K-means}}},
  shorttitle = {Data Clustering},
  author = {Jain, Anil K.},
  year = {2010},
  month = jun,
  journal = {Pattern Recognition Letters},
  series = {Award Winning Papers from the 19th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  volume = {31},
  number = {8},
  pages = {651--666},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2009.09.011},
  urldate = {2025-08-26},
  abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.},
  keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
  file = {/Users/chenguanyu/Zotero/storage/Z4XCJZGM/Jain - 2010 - Data clustering 50 years beyond K-means.pdf;/Users/chenguanyu/Zotero/storage/GCQTWBYP/S0167865509002323.html}
}

@article{malsiner-walliModelbasedClusteringBased2016,
  title = {Model-Based Clustering Based on Sparse Finite {{Gaussian}} Mixtures},
  author = {{Malsiner-Walli}, Gertraud and {Fr{\"u}hwirth-Schnatter}, Sylvia and Gr{\"u}n, Bettina},
  year = {2016},
  month = jan,
  journal = {Statistics and Computing},
  volume = {26},
  number = {1-2},
  eprint = {1606.06828},
  primaryclass = {stat},
  pages = {303--324},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-014-9500-2},
  urldate = {2025-08-29},
  abstract = {In the framework of Bayesian model-based clustering based on a finite mixture of Gaussian distributions, we present a joint approach to estimate the number of mixture components and identify cluster-relevant variables simultaneously as well as to obtain an identified model. Our approach consists in specifying sparse hierarchical priors on the mixture weights and component means. In a deliberately overfitting mixture model the sparse prior on the weights empties superfluous components during MCMC. A straightforward estimator for the true number of components is given by the most frequent number of non-empty components visited during MCMC sampling. Specifying a shrinkage prior, namely the normal gamma prior, on the component means leads to improved parameter estimates as well as identification of cluster-relevant variables. After estimating the mixture model using MCMC methods based on data augmentation and Gibbs sampling, an identified model is obtained by relabeling the MCMC output in the point process representation of the draws. This is performed using \$K\$-centroids cluster analysis based on the Mahalanobis distance. We evaluate our proposed strategy in a simulation setup with artificial data and by applying it to benchmark data sets.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  note = {Comment: 22 pages},
  file = {/Users/chenguanyu/Zotero/storage/WPSDS3V4/Malsiner-Walli et al. - 2016 - Model-based clustering based on sparse finite Gaussian mixtures.pdf;/Users/chenguanyu/Zotero/storage/TJLQ8PRC/1606.html}
}

@article{millerMixtureModelsPrior2018,
  title = {Mixture {{Models With}} a {{Prior}} on the {{Number}} of {{Components}}},
  author = {Miller, Jeffrey W. and Harrison, Matthew T.},
  year = {2018},
  month = jan,
  journal = {Journal of the American Statistical Association},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  urldate = {2025-08-27},
  abstract = {A natural Bayesian approach for mixture models with an unknown number of components is to take the usual finite mixture model with symmetric Dirichlet weights, and put a prior on the number of comp...},
  copyright = {{\copyright} 2018 American Statistical Association},
  langid = {english},
  file = {/Users/chenguanyu/Zotero/storage/ZVE8H9JE/01621459.2016.html}
}

@incollection{mullerBayesianNonparametricMixture2019,
  title = {Bayesian {{Nonparametric Mixture Models}}},
  booktitle = {Handbook of {{Mixture Analysis}}},
  author = {M{\"u}ller, Peter},
  year = {2019},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {This chapter reviews some of the popular Bayesian nonparametric mixture models, starting with the widely used Dirichlet process (DP) mixture models. It provides},
  isbn = {978-0-429-05591-1}
}

@article{nobileBayesianFiniteMixtures2007,
  title = {Bayesian Finite Mixtures with an Unknown Number of Components: The Allocation Sampler},
  shorttitle = {Bayesian Finite Mixtures with an Unknown Number of Components},
  author = {Nobile, Agostino and Fearnside, Alastair T.},
  year = {2007},
  month = jun,
  journal = {Statistics and computing},
  volume = {17},
  number = {2},
  pages = {147--162},
  issn = {0960-3174},
  doi = {10.1007/s11222-006-9014-7},
  abstract = {A new Markov chain Monte Carlo method for the Bayesian analysis of finite mixture distributions with an unknown number of components is presented. The sampler is characterized by a state space consisting only of the number of components and the latent allocation variables. Its main advantage is that it can be used, with minimal changes, for mixtures of components from any parametric family, under the assumption that the component parameters can be integrated out of the model analytically. Artificial and real data sets are used to illustrate the method and mixtures of univariate and of multivariate normals are explicitly considered. The problem of label switching, when parameter inference is of interest, is addressed in a post-processing stage.},
  keywords = {Classification,Galaxy data,Iris data,Label switching,Markov chain Monte Carlo,Multivariate normal mixtures,Normal mixtures,Reversible jump}
}

@article{pollardQuantizationMethodOfkmeans1982,
  title = {Quantization and the Method Ofk-Means},
  author = {Pollard, D.},
  year = {1982},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {199--205},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.1982.1056481},
  urldate = {2025-08-28},
  abstract = {Asymptotic results from the statistical theory of k -means clustering are applied to problems of vector quantization. The behavior of quantizers constructed from long training sequences of data is analyzed by relating it to the consistency problem for k -means.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  note = {[TLDR] Asymptotic results from the statistical theory of k -means clustering are applied to problems of vector quantization and the behavior of quantizers constructed from long training sequences of data is analyzed.}
}

@article{richardsonBayesianAnalysisMixtures,
  title = {On {{Bayesian Analysis}} of {{Mixtures}} with an {{Unknown Number}} of {{Components}}},
  author = {Richardson, Sylvia and Green, Peter J},
  abstract = {New methodology for fully Bayesian mixture analysis is developed, making use of reversiblejump Markov chain Monte Carlo methods that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture. A sample from the full joint distribution of all unknown variables is thereby generated, and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution. The methodology is applied here to the analysis of univariate normal mixtures, using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context.},
  langid = {english},
  file = {/Users/chenguanyu/Zotero/storage/IN9J8T4C/Richardson and Green - On Bayesian Analysis of Mixtures with an Unknown Number of Components.pdf}
}

@article{rigonGeneralizedBayesFramework2023,
  title = {A Generalized {{Bayes}} Framework for Probabilistic Clustering},
  author = {RIGON, {\relax TOMMASO} and HERRING, AMY H. and DUNSON, DAVID B.},
  year = {2023},
  month = sep,
  journal = {Biometrika},
  volume = {110},
  number = {3},
  pages = {559--578},
  issn = {0006-3444},
  doi = {10.1093/biomet/asad004},
  urldate = {2025-08-27},
  abstract = {Loss-based clustering methods, such as k-means clustering and its variants, are standard tools for finding groups in data. However, the lack of quantification of uncertainty in the estimated clusters is a disadvantage. Model-based clustering based on mixture models provides an alternative approach, but such methods face computational problems and are highly sensitive to the choice of kernel. In this article we propose a generalized Bayes framework that bridges between these paradigms through the use of Gibbs posteriors. In conducting Bayesian updating, the loglikelihood is replaced by a loss function for clustering, leading to a rich family of clustering methods. The Gibbs posterior represents a coherent updating of Bayesian beliefs without needing to specify a likelihood for the data, and can be used for characterizing uncertainty in clustering. We consider losses based on Bregman divergence and pairwise similarities, and develop efficient deterministic algorithms for point estimation along with sampling algorithms for uncertainty quantification. Several existing clustering algorithms, including k-means, can be interpreted as generalized Bayes estimators in our framework, and thus we provide a method of uncertainty quantification for these approaches, allowing, for example, calculation of the probability that a data point is well clustered.},
  pmcid = {PMC11840691},
  pmid = {39980604},
  file = {/Users/chenguanyu/Zotero/storage/WEWEH48L/RIGON et al. - 2023 - A generalized Bayes framework for probabilistic clustering.pdf}
}

@article{rousseauAsymptoticBehaviourPosterior2011,
  title = {Asymptotic Behaviour of the Posterior Distribution in Overfitted Mixture Models},
  author = {Rousseau, Judith and Mengersen, Kerrie},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B},
  volume = {73},
  number = {5},
  pages = {689--710},
  publisher = {Royal Statistical Society},
  doi = {10.1111/j.1467-9868.2011.00781.x},
  urldate = {2025-08-27},
  abstract = {In this paper we study the asymptotic behaviour of the posterior distribution in a mixture model when the number of components in the mixture is larger than the true number of components, a situation commonly referred to as overfitted mixture. We prove in particular that quite generally the posterior distribution has a stable and interesting behaviour, since it tends to empty the extra components. This stability is achieved under some restriction on the prior, which can be used as a guideline for choosing the prior. Some simulations are presented to illustrate this behaviour.},
  keywords = {Asymptotic,Bayesian,mixture models,overfitting,posterior concentration},
  file = {/Users/chenguanyu/Zotero/storage/T3B932WH/Rousseau and Mengersen - 2011 - Asymptotic behaviour of the posterior distribution in overfitted mixture models.pdf}
}

@article{vinhInformationTheoreticMeasures,
  title = {Information {{Theoretic Measures}} for {{Clusterings Comparison}}: {{Variants}}, {{Properties}}, {{Normalization}} and {{Correction}} for {{Chance}}},
  author = {Vinh, Nguyen Xuan and Vinh, N X and Epps, Julien and Epps, J and Bailey, James},
  abstract = {Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants.},
  langid = {english},
  file = {/Users/chenguanyu/Zotero/storage/KAMQ2XRI/Vinh et al. - Information Theoretic Measures for Clusterings Comparison Variants, Properties, Normalization and C.pdf}
}

@article{vinhInformationTheoreticMeasures2010,
  title = {Information {{Theoretic Measures}} for {{Clusterings Comparison}}: {{Variants}}, {{Properties}}, {{Normalization}} and {{Correction}} for {{Chance}}},
  shorttitle = {Information {{Theoretic Measures}} for {{Clusterings Comparison}}},
  author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {95},
  pages = {2837--2854},
  issn = {1533-7928},
  urldate = {2025-08-27},
  abstract = {Information theoretic  measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0,1] range better than other normalized variants.},
  file = {/Users/chenguanyu/Zotero/storage/MUXPAXMX/Vinh et al. - 2010 - Information Theoretic Measures for Clusterings Comparison Variants, Properties, Normalization and C.pdf}
}

@article{wadeBayesianClusterAnalysis2018,
  title = {Bayesian {{Cluster Analysis}}: {{Point Estimation}} and {{Credible Balls}} (with {{Discussion}})},
  shorttitle = {Bayesian {{Cluster Analysis}}},
  author = {Wade, Sara and Ghahramani, Zoubin},
  year = {2018},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {2},
  pages = {559--626},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/17-BA1073},
  urldate = {2025-08-26},
  abstract = {Clustering is widely studied in statistics and machine learning, with applications in a variety of fields. As opposed to popular algorithms such as agglomerative hierarchical clustering or k-means which return a single clustering solution, Bayesian nonparametric models provide a posterior over the entire space of partitions, allowing one to assess statistical properties, such as uncertainty on the number of clusters. However, an important problem is how to summarize the posterior; the huge dimension of partition space and difficulties in visualizing it add to this problem. In a Bayesian analysis, the posterior of a real-valued parameter of interest is often summarized by reporting a point estimate such as the posterior mean along with 95\% credible intervals to characterize uncertainty. In this paper, we extend these ideas to develop appropriate point estimates and credible sets to summarize the posterior of the clustering structure based on decision and information theoretic techniques.},
  keywords = {Binder's loss,mixture model,random partition,variation of information},
  file = {/Users/chenguanyu/Zotero/storage/7W6YSPIW/Wade and Ghahramani - 2018 - Bayesian Cluster Analysis Point Estimation and Credible Balls (with Discussion).pdf}
}
