% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/WASABI_multistart.R
\name{WASABI_multistart}
\alias{WASABI_multistart}
\title{Run the WASABI algorithm multiple times with different starting points}
\usage{
WASABI_multistart(cls.draw = NULL, psm = NULL, multi.start = 10, ncores = 1,
                  method.init = c("average", "complete", "fixed", "++",
                                  "random_partition", "+++", "topvi"),
                  add_topvi = TRUE,
                  lb = FALSE, thin.init = NULL, part.init = NULL,
                  method = c("average", "complete", "greedy", "salso"),
                  max.k = NULL, L = 10, max.iter = 30, eps = 0.0001, mini.batch = 0,
                  extra.iter = NULL,
                  swap_countone = FALSE,
                  suppress.comment = TRUE,
                  return_psm = FALSE, seed = NULL, loss = c("VI","Binder","omARI"), a = 1, ...)
}
\arguments{
\item{cls.draw}{A matrix of the MCMC samples of partitions of $n$ data points.}

\item{psm}{The posterior similarity matrix obtained from MCMC samples of partitions stored in \code{cls.draw}.}

\item{multi.start}{Integer, the number of times to run the WASABI algorithm with different starting points.}

\item{L}{Integer, the number of particles to be used in the WASABI approximation.}

\item{ncores}{Integer, the number of cores to use for parallel processing, relies on parallel::mclapply, so can only be used on MacOs and Linux systems. If 1, no parallel processing is used.}

\item{method.init}{Initialization method. Options are "average", "complete", "fixed", "++", "random_partition", "+++", and "topvi".}

\item{add_topvi}{Logical, if TRUE, the "topvi" method is added once to the initialization methods. This method uses the top VI partitions from hierarchical clustering.}

\item{lb}{Logical, if TRUE, the lower bound for the VI is used in methods "average" or "complete".}

\item{thin.init}{Integer, thinning factor for the MCMC samples used to initialize the particles. If NULL, defaults to 10.}

\item{part.init}{A matrix of size \code{L} x \code{n}, containing the initial particles. Needs to be provided when \code{method.init = "fixed"}.}

\item{method}{The method used to find the partition with minimum EVI (minVI partition). Options are "average", "complete", "greedy", and "salso".}

\item{max.k}{Integer, the maximum number of clusters considered in the WASABI approximation (for "average", "complete", "greedy"). If NULL, it is set to the minimum of \code{max(Ks.draw)+10} and \code{ceiling(sqrt(n))}.}

\item{max.iter}{Integer, the maximum number of iterations for the WASABI algorithm.}

\item{eps}{Numeric, the convergence threshold for the WASABI algorithm. The algorithm stops when the difference in Wasserstein distance between two consecutive iterations is less than \code{eps}.}

\item{mini.batch}{Integer, the size of the mini-batch used in the WASABI algorithm. If 0, the full batch is used.}

\item{extra.iter}{Integer, the number of additional iterations to run after the mini-batch optimization. If NULL, defaults to 1 if \code{mini.batch > 0}.}

\item{swap_countone}{Logical, if TRUE, the WASABI algorithm allows swapping of particles with only one sample assigned to them (outlier-check step).}

\item{suppress.comment}{Logical, if TRUE, suppresses the output comments during the WASABI algorithm execution.}

\item{seed}{An optional integer, or a vector of length \code{multi.start} of integers to set the random seed for reproducibility. If NULL, no seed is set.}

\item{loss}{Character, the loss function to be used in the WASABI algorithm. Options are "VI", "Binder", and "omARI". Defaults to "VI".}

\item{return_psm}{Logical, if TRUE, returns the posterior similarity matrix for each particle.}
}
\value{
particles A matrix with \code{L} rows, each containing one of the WASABI particles, ordered by decreasing weight.

EVI A vector of size \code{L}, containing expected VI associated to each particle.

wass.dist A numeric value giving the Wasserstein distance achieved by the WASABI approximation.

part.psm A list of size \code{L}, each element containing the posterior similarity matrix corresponding to the region of attraction of each particle. Only returned if \code{return_psm} is set to TRUE.

part.weights A vector of size \code{L}, containing weight associated to each particle.

draws.assign A vector containing the assignment of each MCMC sample to its closest particle. Its length is equal to the number of rows in \code{cls.draw}.

A list with elements:
\describe{
  \item{particles}{A matrix with \code{L} rows, each containing one of the WASABI particles, ordered by decreasing weight.}
  \item{EVI}{A vector of length \code{L}, containing expected VI associated to each particle.}
  \item{wass.dist}{A numeric value giving the Wasserstein distance achieved by the WASABI approximation.}
  \item{part.psm}{A list of length \code{L}, each element containing the posterior similarity matrix corresponding to the region of attraction of each particle. Only returned if \code{return_psm} is TRUE.}
  \item{part.weights}{A vector of length \code{L}, containing weight associated to each particle.}
  \item{draws.assign}{A vector containing the assignment of each MCMC sample to its closest particle.}
}
}
\description{
The WASABI algorithm can be used to summarize posterior uncertainty in Bayesian clustering by
identifying multiple representative partitions (''particles'') rather than a single
point estimate. It approximates the full posterior over clusterings with a small
set of $L$ weighted partitions, selected to minimize the Wasserstein distance (with
Variation of Information loss) between the posterior and its summary.
\code{WASABI_multistart} runs the algorithm from multiple starting points and
returns the run achieving the best WASABI approximation, i.e. the one minimizing the Wasserstein distance.
}
\details{
Several initialization methods are available:
\itemize{
   \item \code{"average"} and \code{"complete"} initialize the particles using hierarchical clustering (choosing the $L$ ones with smallest EVI);
   \item \code{"fixed"} initializes the algorithm with a set of $L$ partition provided in \code{part.init};
   \item \code{"++"} uses an algorithm similar to k-means++, i.e. promotes diversity among initial centers by iteratively choosing the next center with probability proportional to its VI distance from the closest already chosen center;
   \item \code{"random_partition"} initializes by randomly assigning each data point to one of $L$ groups, and the center for each group is chosen based on these assignments;
   \item \code{"+++"} implements the k-means++ initialization using by choosing from the MCMC samples and the partitions obtained from hierarchical clustering (average and complete);
   \item \code{"topvi"} initializes with the $L$ partitions with smallest EVI, chosen from the ones generated by \code{"average"}, \code{"complete"} and \code{"fixed"} (if \code{part.init} is provided).
}
It is recommended to use random initializations with multiple starts, such as \code{"++"}, \code{"random_partition"}, or \code{"+++"}, to avoid local minima.
The function uses \code{parallel::mclapply} to run the algorithm in parallel, however this is only supported on MacOS and Linux systems.
If you are using Windows, set \code{ncores = 1} to run the algorithm sequentially.

The WASABI algorithm iteratively updates the particles by computing the region of attractions (i.e. the assignment of each MCMC sample to the closest particle), in the N-update step, and finding the minEVI particle for each group, in the VI-search step.
The VI-search step can rely on different algorithms for finding the minEVI particle, depending on the \code{method} argument:
\itemize{
   \item \code{"average"} and \code{"complete"} use hierarchical clustering to find the minEVI particle;
   \item \code{"greedy"} uses the greedy algorithm implemented in \code{MinimiseEPL} of the \code{GreedyEPL} package;
   \item \code{"salso"} uses the \code{salso} package to find the minEVI particle.
}
The recommended method is \code{"salso"}, as it is the most accurate while remaining efficient for larger datasets.
In case of larger datasets, it is recommended to use method \code{"average"}, or the \code{mini.batch} argument to speed up the algorithm.
}
\examples{
\dontrun{
set.seed(123)
mu <- c(-1.1, 1.1)
prop <- c(0.5, 0.5)
n <- 300
components <- sample(1:2, size = n, replace = TRUE, prob = prop)
y <- rnorm(n, mean = mu[components], sd = 1)
est_model <- BNPmix::PYdensity(y = y,
                               mcmc = list(niter = 6000,
                                           nburn = 5000,
                                           model = "LS"),
                               output = list(out_type = "FULL", out_param = TRUE))
cls.draw = est_model$clust
psm=mcclust::comp.psm(cls.draw+1)
# if running WASABI once, a non-random initialization is recommended, such as "topvi" or "average"
out_WASABI <- WASABI_multistart(cls.draw, psm = psm, L = 2, multi.start = 10,
                                method.init = "++", method = "salso", loss = "VI")

# for larger n, it can be faster to use mini.batch
out_WASABI <- WASABI_multistart(cls.draw, psm = psm, L = 2, multi.start = 10,
                                method.init = "topvi", method = "salso",
                                mini.batch = 200, max.iter = 10, extra.iter = 10, loss = "VI")

# Parallel processing is currently supported only on MacOS and Linux systems.
# To activate it, set ncores > 1. If you are using Windows, set ncores = 1.
out_WASABI <- WASABI_multistart(cls.draw, psm = psm, L = 2,
                                ncores = 3, multi.start = 20,
                                method.init = "topvi", method = "salso", loss = "VI")
}
}
\seealso{
WASABI, elbow
}
